{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Machine Learning Course\"\nauthor: \"Stefan Fouche\"\ndate: \"06 January 2017\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n# Important machine learning libraries\n\n```{r}\nrequire(tidyr)\nrequire(caTools)\nrequire(ggplot2)\n# install.packages(\"purrr\")\nrequire(purrr)\nrequire(dplyr)\nrequire(data.table)\nrequire(cluster)\nrequire(e1071)\nrequire(rpart)\n\n```\n\nThis is my Markdown following through the machine learning course\n\n# Part 1 - Data Preprocessing\n\nSummary of things to do when processing the data for machine learning purposes:\n- Import data\n- Deal with NA's\n- Encode categorical/text variables into numbers\n- Split into test and train\n- Apply feature scaling\n\nFor part 1 I will import some data first\n\n```{r}\n#Import the data from 'data.csv'\ndataset <-\n  read.csv(\"Part 1/7_Data.csv\", header = TRUE)\n\n# #My manual control for NA values  \n# meanAge <-\n#   mean(dataset$Age, na.rm = TRUE)\n# dataset$Age[which(is.na(dataset$Age))] <-\n#   meanAge\n#   \n# meanSalary <-\n#   mean(dataset$Salary, na.rm = TRUE)\n# dataset$Salary[which(is.na(dataset$Salary))] <-\n#   meanSalary\n  \n\n#Tutorial method for dealing with NA\ndataset$Age <- ifelse(is.na(dataset$Age),mean(dataset$Age,na.rm = T),dataset$Age)\ndataset$Salary <- ifelse(is.na(dataset$Salary),mean(dataset$Salary,na.rm = T),dataset$Age)\n\nX <-\n  dataset[,-4]\nY <-\n  dataset[, 4]\n\n#Encode the categorical variables into numbers\n\n#Purchased variable\ndataset$Purchased <- ifelse(dataset$Purchased == \"Yes\", 1, 0)\n\ndataset$Country <- dataset$Country %>% factor(levels = c(\"France\", \"Spain\", \"Germany\"),\n                                              labels = c(1, 2, 3))\n\n# #If we want to further spread the Country into clasification (as with logistic regression) I would use this:\n# dataset %>% \n#   spread(Country, Country) %>% \n#   rename(France = 1, Spain = 2, Germany = 3)\n\n```\n\n## Splitting the dataset into a training and test set\n\n```{r}\n#We use caTools\nset.seed(123)\nsplit <- \n  sample.split(dataset$Purchased, SplitRatio = 0.8)\ntraining_set <- subset(dataset, split == T)\ntest_set <- subset(dataset, split == F)\n\n# #Or my own faster way\n# Train <- dataset[split,]\n# Test <- dataset[!split,]\n\n```\n\n## Feature scaling\n\nWhy feature scale?\nBecause the different features (variables of the training data) are not on the same scale and because machine leaarning models use methods such as euclidean distance squaring of parameters may make some more significant than others which we do not want.\n\n- Avoid squared interactions overpowering small bias/observations\n\nHow do we feature scale?\n- Standardisation [x_stand <- (x-mean(x))/std(x)]\n- Normalisation [x_norm <- (x-min(x))/(max(x)-min(x))]\n\n## Scale our features for the data\n\n```{r}\ntraining_set[,2:3] <- scale(training_set[,2:3] )\ntest_set[,2:3]  <- scale(test_set[,2:3] )\n```\n\nHere is how I decided to achieve scaling before seeing the results:\n- My application of applying the mentioned formulas for normalisation and standardisation does not = the scale methods results implying that the scale method is somehow optimised\n- Upon reading the help-text for scale it appears to centre and use a root-mean-squared error instead of a simple standard deviation when standardizing.\n\n## Scale Age\n\n```{r}\n# #Standardised\n# dataset$Age <- \n#   sapply(dataset$Age, function(x) (x-mean(dataset$Age))/sd(dataset$Age))\n# \n# #Normalised\n# dataset$Age <- \n#   sapply(dataset$Age, function(x) (x-min(dataset$Age))/(max(dataset$Age)-min(dataset$Age)))\n\n```\n\n## Scale Salary\n\n```{r}\n# \n# #Standardised\n# dataset$Salary <- \n#   sapply(dataset$Salary, function(x) (x-mean(dataset$Salary))/sd(dataset$Salary))\n# \n# #Normalised\n# dataset$Salary <- \n#   sapply(dataset$Salary, function(x) (x-min(dataset$Salary))/(max(dataset$Salary)-min(dataset$Salary)))\n```\n\n# Part 2 - Regression\n\n```{r}\n\nset.seed(123)\n\n```\n\n## Simple Linear Regression\n\nFirst we fit a linear model to the data\n\n```{r}\ndataset <- \nread.csv(file = \"Part2_Regression/Simple_Linear_Regression/Salary_Data.csv\")\n\nsplit <- \n  sample.split(dataset$Salary, SplitRatio = 2/3)\n\ntraining_set <- \n  dataset[split,]\n\ntest_set <- \n  dataset[!split,]\n\nregressor <- \n  lm(formula = Salary~., training_set)\n\n```\n\nNow we Predict our values using the test data (me)\n\n```{r}\n\ny_pred <- \n  predict(regressor, newdata = test_set) %>% \n  data.frame\n\n#Plot the predictions over the test\nggplot() +\n  geom_point(data = test_set, aes(x = test_set$YearsExperience,\n                                  y = test_set$Salary)) +\n  geom_line(data = y_pred, aes(x = test_set$YearsExperience,\n                               y = y_pred$.))+\n  stat_smooth()\n\n\n```\n\nThe prediction is obviously pretty good since the data was mostly linear already.\n\n## Multiple linear regression\n\nRead in the data:\n\n```{r}\n\ndataset <- \nread.csv(\"Part2_Regression/Multiple-Linear-Regression/Multiple_Linear_Regression/50_Startups.csv\")\ndataset\n\n```\n\npre-process the data\n\n```{r}\n\ndataset$State <- dataset$State %>% factor(levels = c(\"New York\", \"California\", \"Florida\"),\nlabels = c(1, 2, 3))\ndataset\n```\n\n\nSplit the data:\n\n```{r}\n\nsplit <- \n  sample.split(dataset$Profit, SplitRatio = 8/10)\n\ntraining_set <- \n  dataset[split,]\ntraining_set\n\ntest_set <- \n  dataset[!split,]\ntest_set\n\n```\n\nRun the multiple linear regression model and compare the results\n\n```{r}\n\nregressor <- \n  lm(formula = Profit~., training_set)\n\nsummary(regressor)\n\nProfit_pred <- \n  predict(regressor, test_set)\n\nCompare_df <- \n  cbind(test_set, Profit_pred)\n\nCompare_df %>% mutate(Accuracy = abs(Profit_pred-Profit)/Profit)\n\n```\n\nNow preoceed with backward elimination\n\n```{r}\np_value <- 0.05\n\n#Remove the State variable\nregressor <- \n  lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend, dataset)\n\nsummary(regressor) \n\n#Remove the Administration variable\nregressor <- \n  lm(formula = Profit ~ R.D.Spend + Marketing.Spend, dataset)\n\nsummary(regressor) \n\n#Remove the State & Marketing spend variable\nregressor <- \n  lm(formula = Profit ~ R.D.Spend + Administration, dataset)\n\nsummary(regressor) \n\n```\n\nWe can also use the purrr package to run the lm model on all the columns! I will do this with a polynomial model (overfit and predict each variable, only usefull if we need to estimate the profit given a set value(s) for a variable(s) )\n\n```{r}\n# Tired, will do later\n```\n\n## Polynomial Regression\n\nRead in the data  \n\n```{r}\n\ndataset <- \nread.csv(\"Part3_Polynomial_Regression/Polynomial_Regression/Position_Salaries.csv\")\n\n```\n\nCreate a linear regression model to compare (data is exponential)\n\n```{r}\nregressor <- lm(formula = Salary~Level, dataset)\n\nLinear_regressor <- \n  predict(regressor, dataset) %>% data.frame()\n\nggplot() +\n  geom_point(data = dataset, aes(x = dataset$Level,\n                                  y = dataset$Salary)) +\n  geom_line(data = dataset, aes(x = dataset$Level,\n                               y = Linear_regressor$.))\n\nValue_Prediction <- \n  predict(regressor, data.frame(Level = 6.5))\n```\n\nNow we create the polynomial regressor.\nThe way they do it is actually very rigid and arduous so I used the poly function which enables a more accurate and also easier polynomial regression (pretty awesome)\n\n```{r}\nPoly_Regressor <- \n  lm(formula = Salary~poly(Level,9), dataset)\nPoly_predict <- \n  predict(Poly_Regressor, dataset) %>% data.frame()\n\nggplot() +\n  geom_point(data = dataset, aes(x = dataset$Level,\n                                  y = dataset$Salary)) +\n  geom_line(data = y_pred, aes(x = dataset$Level,\n                               y = Poly_predict$.))\n\nValue_Prediction <- \n  predict(Poly_Regressor, data.frame(Level = 6.5))\n\nValue_Prediction\n```\n\nWhat about a log transform model?\n\n```{r}\nLog_Regressor <- \n  lm(formula = log(Salary)~Level, dataset)\n\nLog_prediction <- \n  exp(predict(Log_Regressor, dataset)) %>% data.frame()\n\nggplot() +\n  geom_point(data = dataset, aes(x = dataset$Level,\n                                  y = dataset$Salary)) +\n  geom_line(data = y_pred, aes(x = dataset$Level,\n                               y = Log_prediction$.))\n\n# TruthOrBluff <- \n#   Log_prediction[which(Log_prediction$. <= 160000),1]\n```\n\nActualy my idea of a log regression is probably not a bad method if you require a smooth line without a high order polynomial which **should increase R-Squared?**.\n\n## Support vector machine regression\n\nSVR is very straight forward to implement. We call the svm() function from the e1071 package similar to lm(). The function will automatically use classification algorythms if we are regressing a factor variable in the formula. It is still a good idea to specify the method outright.  \n\nUse the following main types:  \n- type = C-classification for classification  \n- type = eps-regression for regression of numeric variables. \n\nRemember to do the usual data preparation; tokenize factors, scale variables. This will give you the best predictions.  \n\n### Load the data\n\n```{r}\ndata <- read.csv(file = \"Part2_Regression/SVR/SVR/Position_Salaries.csv\")\nhead(data)\n```\n\n### Create SVR regressor and prediction\n\n```{r}\nregressor <- svm(formula = Salary ~ .,data = data, type = \"eps-regression\")\ny_pred <- predict(regressor, data) %>% data.frame()\n```\n\n### Plot the regression\n\n```{r}\n# Value_Prediction <-\n#   predict(regressor, data.frame(Level = 6.5, Position = modelr::typical(data$Position)))\n\nggplot() +\n  geom_point(data = data, aes(x = data$Level,\n                                  y = data$Salary)) +\n  geom_line(data = y_pred, aes(x = data$Level,\n                               y = y_pred))\n  # geom_point(data = y_pred, aes(x = 6.5,\n  #                              y = Value_Prediction))\n\n```\n\n## Regression Trees\n\n### Intuition\n\nCART stands for classification and regression trees.  \nThere are, thereforem, 2 types of  trees (as usual).  \n\nWatch the intuition video here: https://www.udemy.com/machinelearning/learn/v4/t/lecture/5732730?start=0  \n\nRegression trees are based on information entropy. Entropy is a measure of uncertainty or the value of an additional data point in terms of information. \n\nEntropy is defined as $$E(-ln(P(X)))$$. It is often taken with base 2 in which case it is defined as bit entropy. Base 2 is computationally more efficient and often better leverages in binary outcomes. In the case of a bernouli process the entropy would be: $$\\sum -P(x_i)ln(P(x_i)) = \\sum -1/2ln(1/2)$$. The entropy is maximised when the odds are 1/2 because then we are the least certain  \n\nThe algorythm will devide the data into branches where the entropy is above a certain threshold. Once it has created enough branches it will make the regression prediction based on the average of the predictions of each branch depending on which branch the values lie for which you are predicting.  \n\n### Load the data\n\n```{r}\ndataset <- \nread.csv(file = \"Part2_Regression/Decision_Tree_Regression/Decision_Tree_Regression/Position_Salaries.csv\")\n```\n\n### Run the regression\n\n```{r}\nregressor <- rpart(formula = Salary ~ ., data = dataset)\n\ny_predict <- predict(regressor, newdata = dataset) %>% data.frame\n```\n\n### Plot\n\n```{r}\nggplot() +\n  geom_point(data = dataset, aes(x = dataset$Level,\n                                  y = dataset$Salary)) +\n  geom_line(data = y_predict, aes(x = dataset$Level,\n                               y = y_predict))\n```\n\nThere are no clear decision trees formed so the model  just took an average in 1 branch.  \n\nLet's create more branches:\n\n```{r}\nregressor <- rpart(formula = Salary ~ ., data = dataset, control = rpart.control(minsplit = 1))\n\ny_predict <- predict(regressor, newdata = dataset) %>% data.frame\n\nggplot() +\n  geom_point(data = dataset, aes(x = dataset$Level,\n                                  y = dataset$Salary)) +\n  geom_line(data = y_predict, aes(x = dataset$Level,\n                               y = y_predict))\n```\n\nIn reality the decision tree is actually just giving an average for each branch. So it doesn't produce a smooth graph. It will return an average based on which branch/decision segment your independant predictor variables lie. To show this we predict not 1 point for each level but all point in an almost continuous sequence between the min and max value of level:\n\n```{r}\nx_grid <- seq(from = min(dataset$Level), to = max(dataset$Level), 0.01)\n\ny_predict <- predict(regressor, newdata = data.frame(Level = x_grid)) %>% data.frame\n\nggplot() +\n  geom_point(data = dataset, aes(x = dataset$Level,\n                                  y = dataset$Salary)) +\n  geom_line(data = y_predict, aes(x = x_grid,\n                               y = y_predict))\n\n```\n\n# Part 4 - Clustering\n\n# K-means\n\n## Load the data\n\n```{r}\ndataset <-\n  data.table::fread(\"Part_4_Clustering/K_Means/K_Means/Mall_Customers.csv\")\n  # read.csv(file = \"Part_4_Clustering/K_Means/K_Means/Mall_Customers.csv\")\n\n```\n\n## Find best number of clusters using elbow method\n\n```{r}\nsubset <- \n  dataset %>% select(`Annual Income (k$)`, `Spending Score (1-100)`)\n\nwcss <- vector()\n\nfor(i in 1:50){\n  wcss[i] <- sum(kmeans(subset,i)$withinss)\n}\n\nwcss %>% plot\n\n```\n\nlooks like ~5 clusters will give us a meaningfull number of clusters\n\n## Create the clusters\n\nOn variables income, spend score:  \n\n```{r}\n\ncluster_1 <- kmeans(dataset %>% select(4:5), 5, iter.max = 1000, nstart = 10)\n\n```\n\n## Plot the clusters\n\n```{r}\nclusplot(\n  dataset %>% select(4:5),\n  cluster_1$cluster,\n  lines = 0,\n  shade = FALSE,\n  color = TRUE,\n  labels = 1,\n  plotchar = FALSE,\n  span = TRUE\n  # main = \"main title\",\n  # xlab = \"x title\",\n  # ylab = \"y title\"\n)\n```\n\nApparently the k-means algorythm thinks these 2 columns describe all the variability. So a regression with only these factors should produce a good R^2   \n\nLet's try adding some more columns and see how the plot and clustering algorythm behaves:  \n\n_To include all the other variables we will have to tokenize the Genre column into a true/false 1/0 so that the algorythm can calculate euclidean distance, one might also want to normalize the variables_\n\n```{r}\n\nsubset_2 <- \n  dataset %>% \n  select(Genre:`Spending Score (1-100)`) %>% \n  mutate(Genre = factor(Genre, labels = c(0,1),levels = c(\"Female\",\"Male\")))\n  # mutate(Genre = factor(Genre, levels = c(0,1),labels = c(\"Female\",\"Male\")))\n\n\nwcss_2 <- vector()\n\nfor(i in 1:50){\n  wcss_2[i] <- sum(kmeans(subset_2,i)$withinss)\n}\n\nwcss_2 %>% plot\n\n# Quicly look at points and gradients/skewness\nwcss_2 %>% \n  tbl_df() %>% \n  arrange(-value) %>% \n  mutate(clusters = seq_along(value),\n         gradient = lag(value)-value,\n         gradient_rate_change = lag(gradient)-gradient) %>% \n  mutate_all(round)\n\n# Using the 2nd derivitive as an aproximation we use 7 clusters as the optimal number\n\ncluster_2 <- kmeans(subset_2, 7, iter.max = 1000, nstart = 10)\n\n# clusplot(\n#   subset_2,\n#   cluster_2$cluster,\n#   lines = 0,\n#   shade = FALSE,\n#   color = TRUE,\n#   labels = 1,\n#   plotchar = FALSE,\n#   span = TRUE,\n#   main = \"main title\",\n#   xlab = \"x title\",\n#   ylab = \"y title\"\n# )\n```\n\nWhen we look at the kmeans output it has clearly used all the new columns. But the clusplot function can only plot 2 variables  \n\n# Hierarchical clustering\n\nThere are broad approaches to H-clustering:  \n  - Agglomerative  \n  - Divisive  \n\n_Agglomerative_ starts from the bottom and builds everything up (using a dendogram), and _Divisive_ is the opposite.    \n  \n_HC uses distance between clusters_.  \nThis can be defined in different ways since a cluster is a group of points!  \n\nSteps:  \n  1. Make a cluster for each point  \n  2. Make n-1 clusters by grouping together the closest clusters  \n  3. repeat step 2 untill you have 1 cluster left  \n  \n## Dendograms Agglomerative\n\nA dendogram is a plot of distances for each cluster from another showing how clusters where connected given their distances  \n\nFor _Agglomerative_ the dendogram will build larger clusters from smaller clusters like building a tower. \n\n## Choosing the number of clusters using the dendogram\n\nBy looking at the dendogram we can determine the number of clusters we want to use by identifying some threshold on vertical distance that we do not want to cross.  \n\nHere vertical distance refers to the distance between the clusters being joined on the current iteration \n\n_Rule of thumb:_ make the threshold somewhere in the height of the cluster with the largest vertical distance. Repeat this process untill the next vertical distance is not significantly larger than the previous\n\n## Read in the data\n\n```{r}\ndataset <- \n  fread(\"Part_4_Clustering/Hierarchical-Clustering/Hierarchical_Clustering/Mall_Customers.csv\")\n\nsubset <- \n  dataset %>% select(`Annual Income (k$)`,`Spending Score (1-100)`)\n```\n\n## Plot dendogram\n\nmethod = \"ward.D\" will minimise each clusters within_variance\n\n```{r}\nHC <- hclust(d = dist(subset, method = \"euclidean\") ,\n                    method = \"ward.D\")\n\nHC %>% plot\n# HC %>% plot(ylim=c(0, 200) )\n```\n\nWe can clearly see from this dendogram that the threshold will be pretty low  \n\n## Choose number of clusters\n\nWe now cut the dendogram tree and choos our number of clusters\n\n```{r}\nHC_clusters <- \nHC %>% cutree(k = 5) # 5 clusters\n```\n\n## Plot the HC results\n\n```{r}\nclusplot(\n  subset,\n  HC_clusters,\n  lines = 0,\n  shade = FALSE,\n  color = TRUE,\n  labels = 1,\n  plotchar = FALSE,\n  span = TRUE,\n  main = \"main title\",\n  xlab = \"x title\",\n  ylab = \"y title\"\n)\n```\n\n# Playground\n\n```{r}\n\n\n```\n\n",
    "created" : 1497984287770.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "0|3|5|0|\n7|27|9|0|\n11|38|28|0|\n29|29|144|0|\n147|6|151|0|\n153|27|195|0|\n196|29|284|0|\n285|24|355|0|\n356|36|395|0|\n398|13|410|0|\n411|17|417|0|\n418|22|425|0|\n428|6|434|0|\n467|21|468|0|\n469|9|578|0|\n579|25|659|0|\n660|12|667|0|\n",
    "hash" : "4006451372",
    "id" : "EFF6D459",
    "lastKnownWriteTime" : 1502665936,
    "last_content_update" : 1502665936394,
    "path" : "C:/Users/User-pc/Desktop/Work/Machine Learning course in R and Python/Machine Learning Course/Machine Learning Course.Rmd",
    "project_path" : "Machine Learning Course.Rmd",
    "properties" : {
        "docOutlineVisible" : "0",
        "last_setup_crc32" : "A3A2C1A0bb338d19"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}