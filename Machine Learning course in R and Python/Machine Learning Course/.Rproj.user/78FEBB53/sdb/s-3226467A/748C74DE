{
    "collab_server" : "",
    "contents" : "---\ntitle: \"Machine Learning Course\"\nauthor: \"Stefan Fouche\"\ndate: \"06 January 2017\"\noutput: html_document\n---\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE)\n```\n\n# Important machine learning libraries\n\n```{r}\nrequire(dplyr)\nrequire(tidyr)\nrequire(caTools)\nrequire(ggplot2)\n# install.packages(\"purrr\")\nrequire(purrr)\n\n```\n\nThis is my Markdown following through the machine learning course\n\n# Part 1 - Data Preprocessing\n\nSummary of things to do when processing the data for machine learning purposes:\n- Import data\n- Deal with NA's\n- Encode categorical/text variables into numbers\n- Split into test and train\n- Apply feature scaling\n\nFor part 1 I will import some data first\n\n```{r}\n#Import the data from 'data.csv'\ndataset <-\n  read.csv(\"Part 1/7_Data.csv\", header = TRUE)\n\n# #My manual control for NA values  \n# meanAge <-\n#   mean(dataset$Age, na.rm = TRUE)\n# dataset$Age[which(is.na(dataset$Age))] <-\n#   meanAge\n#   \n# meanSalary <-\n#   mean(dataset$Salary, na.rm = TRUE)\n# dataset$Salary[which(is.na(dataset$Salary))] <-\n#   meanSalary\n  \n\n#Tutorial method for dealing with NA\ndataset$Age <- ifelse(is.na(dataset$Age),mean(dataset$Age,na.rm = T),dataset$Age)\ndataset$Salary <- ifelse(is.na(dataset$Salary),mean(dataset$Salary,na.rm = T),dataset$Age)\n\nX <-\n  dataset[,-4]\nY <-\n  dataset[, 4]\n\n#Encode the categorical variables into numbers\n\n#Purchased variable\ndataset$Purchased <- ifelse(dataset$Purchased == \"Yes\", 1, 0)\n\ndataset$Country <- dataset$Country %>% factor(levels = c(\"France\", \"Spain\", \"Germany\"),\n                                              labels = c(1, 2, 3))\n\n# #If we want to further spread the Country into clasification (as with logistic regression) I would use this:\n# dataset %>% \n#   spread(Country, Country) %>% \n#   rename(France = 1, Spain = 2, Germany = 3)\n\n```\n\n## Splitting the dataset into a training and test set\n\n```{r}\n#We use caTools\nset.seed(123)\nsplit <- \n  sample.split(dataset$Purchased, SplitRatio = 0.8)\ntraining_set <- subset(dataset, split == T)\ntest_set <- subset(dataset, split == F)\n\n# #Or my own faster way\n# Train <- dataset[split,]\n# Test <- dataset[!split,]\n\n```\n\n## Feature scaling\n\nWhy feature scale?\nBecause the different features (variables of the training data) are not on the same scale and because machine leaarning models use methods such as euclidean distance squaring of parameters may make some more significant than others which we do not want.\n\n- Avoid squared interactions overpowering small bias/observations\n\nHow do we feature scale?\n- Standardisation [x_stand <- (x-mean(x))/std(x)]\n- Normalisation [x_norm <- (x-min(x))/(max(x)-min(x))]\n\n## Scale our features for the data\n\n```{r}\ntraining_set[,2:3] <- scale(training_set[,2:3] )\ntest_set[,2:3]  <- scale(test_set[,2:3] )\n```\n\nHere is how I decided to achieve scaling before seeing the results:\n- My application of applying the mentioned formulas for normalisation and standardisation does not = the scale methods results implying that the scale method is somehow optimised\n- Upon reading the help-text for scale it appears to centre and use a root-mean-squared error instead of a simple standard deviation when standardizing.\n\n## Scale Age\n\n```{r}\n# #Standardised\n# dataset$Age <- \n#   sapply(dataset$Age, function(x) (x-mean(dataset$Age))/sd(dataset$Age))\n# \n# #Normalised\n# dataset$Age <- \n#   sapply(dataset$Age, function(x) (x-min(dataset$Age))/(max(dataset$Age)-min(dataset$Age)))\n\n```\n\n## Scale Salary\n\n```{r}\n# \n# #Standardised\n# dataset$Salary <- \n#   sapply(dataset$Salary, function(x) (x-mean(dataset$Salary))/sd(dataset$Salary))\n# \n# #Normalised\n# dataset$Salary <- \n#   sapply(dataset$Salary, function(x) (x-min(dataset$Salary))/(max(dataset$Salary)-min(dataset$Salary)))\n```\n\n# Part 2 - Regression\n\n```{r}\n\nset.seed(123)\n\n```\n\n## Simple Linear Regression\n\nFirst we fit a linear model to the data\n\n```{r}\ndataset <- \nread.csv(file = \"Part2_Regression/Simple_Linear_Regression/Salary_Data.csv\")\n\nsplit <- \n  sample.split(dataset$Salary, SplitRatio = 2/3)\n\ntraining_set <- \n  dataset[split,]\n\ntest_set <- \n  dataset[!split,]\n\nregressor <- \n  lm(formula = Salary~., training_set)\n\n```\n\nNow we Predict our values using the test data (me)\n\n```{r}\n\ny_pred <- \n  predict(regressor, newdata = test_set) %>% \n  data.frame\n\n#Plot the predictions over the test\nggplot() +\n  geom_point(data = test_set, aes(x = test_set$YearsExperience,\n                                  y = test_set$Salary)) +\n  geom_line(data = y_pred, aes(x = test_set$YearsExperience,\n                               y = y_pred$.))+\n  stat_smooth()\n\n\n```\n\nThe prediction is obviously pretty good since the data was mostly linear already.\n\n## Multiple linear regression\n\nRead in the data:\n\n```{r}\n\ndataset <- \nread.csv(\"Part2_Regression/Multiple-Linear-Regression/Multiple_Linear_Regression/50_Startups.csv\")\ndataset\n\n```\n\npre-process the data\n\n```{r}\n\ndataset$State <- dataset$State %>% factor(levels = c(\"New York\", \"California\", \"Florida\"),\nlabels = c(1, 2, 3))\ndataset\n```\n\n\nSplit the data:\n\n```{r}\n\nsplit <- \n  sample.split(dataset$Profit, SplitRatio = 8/10)\n\ntraining_set <- \n  dataset[split,]\ntraining_set\n\ntest_set <- \n  dataset[!split,]\ntest_set\n\n```\n\nRun the multiple linear regression model and compare the results\n\n```{r}\n\nregressor <- \n  lm(formula = Profit~., training_set)\n\nsummary(regressor)\n\nProfit_pred <- \n  predict(regressor, test_set)\n\nCompare_df <- \n  cbind(test_set, Profit_pred)\n\nCompare_df %>% mutate(Accuracy = abs(Profit_pred-Profit)/Profit)\n\n```\n\nNow preoceed with backward elimination\n\n```{r}\np_value <- 0.05\n\n#Remove the State variable\nregressor <- \n  lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend, dataset)\n\nsummary(regressor) \n\n#Remove the Administration variable\nregressor <- \n  lm(formula = Profit ~ R.D.Spend + Marketing.Spend, dataset)\n\nsummary(regressor) \n\n#Remove the State & Marketing spend variable\nregressor <- \n  lm(formula = Profit ~ R.D.Spend + Administration, dataset)\n\nsummary(regressor) \n\n```\n\nWe can also use the purrr package to run the lm model on all the columns! I will do this with a polynomial model (overfit and predict each variable, only usefull if we need to estimate the profit given a set value(s) for a variable(s) )\n\n```{r}\n# Tired, will do later\n```\n\n## Polynomial Regression\n\nRead in the data\n```{r}\n\ndataset <- \nread.csv(\"Part3_Polynomial_Regression/Polynomial_Regression/Position_Salaries.csv\")\n\n```\n\nCreate a linear regression model to compare (data is exponential)\n\n```{r}\nregressor <- lm(formula = Salary~Level, dataset)\n\nLinear_regressor <- \n  predict(regressor, dataset) %>% data.frame()\n\nggplot() +\n  geom_point(data = dataset, aes(x = dataset$Level,\n                                  y = dataset$Salary)) +\n  geom_line(data = dataset, aes(x = dataset$Level,\n                               y = Linear_regressor$.))\n\nValue_Prediction <- \n  predict(regressor, data.frame(Level = 6.5))\n```\n\nNow we create the polynomial regressor.\nThe way they do it is actually very poor and arduous so I used the poly function which enables a more accurate and also easier polynomial regression (pretty awesome)\n\n```{r}\nPoly_Regressor <- \n  lm(formula = Salary~poly(Level,9), dataset)\nPoly_predict <- \n  predict(Poly_Regressor, dataset) %>% data.frame()\n\nggplot() +\n  geom_point(data = dataset, aes(x = dataset$Level,\n                                  y = dataset$Salary)) +\n  geom_line(data = y_pred, aes(x = dataset$Level,\n                               y = Poly_predict$.))\n\nValue_Prediction <- \n  predict(Poly_Regressor, data.frame(Level = 6.5))\n\nValue_Prediction\n```\n\nWhat about a log transform model?\n\n```{r}\nLog_Regressor <- \n  lm(formula = log(Salary)~Level, dataset)\n\nLog_prediction <- \n  exp(predict(Log_Regressor, dataset)) %>% data.frame()\n\nggplot() +\n  geom_point(data = dataset, aes(x = dataset$Level,\n                                  y = dataset$Salary)) +\n  geom_line(data = y_pred, aes(x = dataset$Level,\n                               y = Log_prediction$.))\n\n# TruthOrBluff <- \n#   Log_prediction[which(Log_prediction$. <= 160000),1]\n```\n\nActualy my idea of a log regression is probably not a bad method if you require a smooth line without a high order polynomial which **should increase R-Squared?**.\n\n# Playground\n\n```{r}\niris %>% nest(-Species) %>% .[[2]] -> List_df\nchickwts %>% nest(weight)\n\nif (require(\"gapminder\")) {\n  gapminder %>%\n    group_by(country, continent) %>%\n    nest()\n\n  gapminder %>%\n    nest(-country, -continent)\n}\n```\n\n",
    "created" : 1483667314435.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2589650355",
    "id" : "748C74DE",
    "lastKnownWriteTime" : 1486592670,
    "last_content_update" : 1486592670818,
    "path" : "C:/Users/QTS/Desktop/Machine Learning course in R and Python/Machine Learning Course/Machine Learning Course.Rmd",
    "project_path" : "Machine Learning Course.Rmd",
    "properties" : {
        "last_setup_crc32" : "",
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_markdown"
}