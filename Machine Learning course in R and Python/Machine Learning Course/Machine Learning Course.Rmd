---
title: "Machine Learning Course"
author: "Stefan Fouche"
date: "06 January 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Important machine learning libraries

```{r}
require(tidyr)
require(caTools)
require(ggplot2)
# install.packages("purrr")
require(purrr)
require(dplyr)
require(data.table)
require(cluster)
require(e1071)
require(rpart)

```

This is my Markdown following through the machine learning course  

# Progress

- Need write up on
    - Part 3; classification  
    - Part 6, section 28; Thompson Sampling 
    - Part 7; Natural language processing 
    - Part 8; Deep learning 
    - Part 9, section 35 & 36; Dimensionality reduction 

# Part 1 - Data Preprocessing

Summary of things to do when processing the data for machine learning purposes:
- Import data
- Deal with NA's
- Encode categorical/text variables into numbers
- Split into test and train
- Apply feature scaling

For part 1 I will import some data first

```{r}
#Import the data from 'data.csv'
dataset <-
  read.csv("Part 1/7_Data.csv", header = TRUE)

# #My manual control for NA values  
# meanAge <-
#   mean(dataset$Age, na.rm = TRUE)
# dataset$Age[which(is.na(dataset$Age))] <-
#   meanAge
#   
# meanSalary <-
#   mean(dataset$Salary, na.rm = TRUE)
# dataset$Salary[which(is.na(dataset$Salary))] <-
#   meanSalary
  

#Tutorial method for dealing with NA
dataset$Age <- ifelse(is.na(dataset$Age),mean(dataset$Age,na.rm = T),dataset$Age)
dataset$Salary <- ifelse(is.na(dataset$Salary),mean(dataset$Salary,na.rm = T),dataset$Age)

X <-
  dataset[,-4]
Y <-
  dataset[, 4]

#Encode the categorical variables into numbers

#Purchased variable
dataset$Purchased <- ifelse(dataset$Purchased == "Yes", 1, 0)

dataset$Country <- dataset$Country %>% factor(levels = c("France", "Spain", "Germany"),
                                              labels = c(1, 2, 3))

# #If we want to further spread the Country into clasification (as with logistic regression) I would use this:
# dataset %>% 
#   spread(Country, Country) %>% 
#   rename(France = 1, Spain = 2, Germany = 3)

```

## Splitting the dataset into a training and test set

```{r}
#We use caTools
set.seed(123)
split <- 
  sample.split(dataset$Purchased, SplitRatio = 0.8)
training_set <- subset(dataset, split == T)
test_set <- subset(dataset, split == F)

# #Or my own faster way
# Train <- dataset[split,]
# Test <- dataset[!split,]

```

## Feature scaling

Why feature scale?
Because the different features (variables of the training data) are not on the same scale and because machine leaarning models use methods such as euclidean distance squaring of parameters may make some more significant than others which we do not want.

- Avoid squared interactions overpowering small bias/observations

How do we feature scale?
- Standardisation [x_stand <- (x-mean(x))/std(x)]
- Normalisation [x_norm <- (x-min(x))/(max(x)-min(x))]

## Scale our features for the data

```{r}
training_set[,2:3] <- scale(training_set[,2:3] )
test_set[,2:3]  <- scale(test_set[,2:3] )
```

Here is how I decided to achieve scaling before seeing the results:
- My application of applying the mentioned formulas for normalisation and standardisation does not = the scale methods results implying that the scale method is somehow optimised
- Upon reading the help-text for scale it appears to centre and use a root-mean-squared error instead of a simple standard deviation when standardizing.

## Scale Age

```{r}
# #Standardised
# dataset$Age <- 
#   sapply(dataset$Age, function(x) (x-mean(dataset$Age))/sd(dataset$Age))
# 
# #Normalised
# dataset$Age <- 
#   sapply(dataset$Age, function(x) (x-min(dataset$Age))/(max(dataset$Age)-min(dataset$Age)))

```

## Scale Salary

```{r}
# 
# #Standardised
# dataset$Salary <- 
#   sapply(dataset$Salary, function(x) (x-mean(dataset$Salary))/sd(dataset$Salary))
# 
# #Normalised
# dataset$Salary <- 
#   sapply(dataset$Salary, function(x) (x-min(dataset$Salary))/(max(dataset$Salary)-min(dataset$Salary)))
```

# Part 2 - Regression

```{r}

set.seed(123)

```

## Simple Linear Regression

First we fit a linear model to the data

```{r}
dataset <- 
read.csv(file = "Part2_Regression/Simple_Linear_Regression/Salary_Data.csv")

split <- 
  sample.split(dataset$Salary, SplitRatio = 2/3)

training_set <- 
  dataset[split,]

test_set <- 
  dataset[!split,]

regressor <- 
  lm(formula = Salary~., training_set)

```

Now we Predict our values using the test data (me)

```{r}

y_pred <- 
  predict(regressor, newdata = test_set) %>% 
  data.frame

#Plot the predictions over the test
ggplot() +
  geom_point(data = test_set, aes(x = test_set$YearsExperience,
                                  y = test_set$Salary)) +
  geom_line(data = y_pred, aes(x = test_set$YearsExperience,
                               y = y_pred$.))+
  stat_smooth()


```

The prediction is obviously pretty good since the data was mostly linear already.

## Multiple linear regression

Read in the data:

```{r}

dataset <- 
read.csv("Part2_Regression/Multiple-Linear-Regression/Multiple_Linear_Regression/50_Startups.csv")
dataset

```

pre-process the data

```{r}

dataset$State <- dataset$State %>% factor(levels = c("New York", "California", "Florida"),
labels = c(1, 2, 3))
dataset
```


Split the data:

```{r}

split <- 
  sample.split(dataset$Profit, SplitRatio = 8/10)

training_set <- 
  dataset[split,]
training_set

test_set <- 
  dataset[!split,]
test_set

```

Run the multiple linear regression model and compare the results

```{r}

regressor <- 
  lm(formula = Profit~., training_set)

summary(regressor)

Profit_pred <- 
  predict(regressor, test_set)

Compare_df <- 
  cbind(test_set, Profit_pred)

Compare_df %>% mutate(Accuracy = abs(Profit_pred-Profit)/Profit)

```

Now preoceed with backward elimination

```{r}
p_value <- 0.05

#Remove the State variable
regressor <- 
  lm(formula = Profit ~ R.D.Spend + Administration + Marketing.Spend, dataset)

summary(regressor) 

#Remove the Administration variable
regressor <- 
  lm(formula = Profit ~ R.D.Spend + Marketing.Spend, dataset)

summary(regressor) 

#Remove the State & Marketing spend variable
regressor <- 
  lm(formula = Profit ~ R.D.Spend + Administration, dataset)

summary(regressor) 

```

We can also use the purrr package to run the lm model on all the columns! I will do this with a polynomial model (overfit and predict each variable, only usefull if we need to estimate the profit given a set value(s) for a variable(s) )

```{r}
# Tired, will do later
```

## Polynomial Regression

Read in the data  

```{r}

dataset <- 
read.csv("Part3_Polynomial_Regression/Polynomial_Regression/Position_Salaries.csv")

```

Create a linear regression model to compare (data is exponential)

```{r}
regressor <- lm(formula = Salary~Level, dataset)

Linear_regressor <- 
  predict(regressor, dataset) %>% data.frame()

ggplot() +
  geom_point(data = dataset, aes(x = dataset$Level,
                                  y = dataset$Salary)) +
  geom_line(data = dataset, aes(x = dataset$Level,
                               y = Linear_regressor$.))

Value_Prediction <- 
  predict(regressor, data.frame(Level = 6.5))
```

Now we create the polynomial regressor.
The way they do it is actually very rigid and arduous so I used the poly function which enables a more accurate and also easier polynomial regression (pretty awesome)

```{r}
Poly_Regressor <- 
  lm(formula = Salary~poly(Level,9), dataset)
Poly_predict <- 
  predict(Poly_Regressor, dataset) %>% data.frame()

ggplot() +
  geom_point(data = dataset, aes(x = dataset$Level,
                                  y = dataset$Salary)) +
  geom_line(data = y_pred, aes(x = dataset$Level,
                               y = Poly_predict$.))

Value_Prediction <- 
  predict(Poly_Regressor, data.frame(Level = 6.5))

Value_Prediction
```

What about a log transform model?

```{r}
Log_Regressor <- 
  lm(formula = log(Salary)~Level, dataset)

Log_prediction <- 
  exp(predict(Log_Regressor, dataset)) %>% data.frame()

ggplot() +
  geom_point(data = dataset, aes(x = dataset$Level,
                                  y = dataset$Salary)) +
  geom_line(data = y_pred, aes(x = dataset$Level,
                               y = Log_prediction$.))

# TruthOrBluff <- 
#   Log_prediction[which(Log_prediction$. <= 160000),1]
```

Actualy my idea of a log regression is probably not a bad method if you require a smooth line without a high order polynomial which **should increase R-Squared?**.

## Support vector machine regression

SVR is very straight forward to implement. We call the svm() function from the e1071 package similar to lm(). The function will automatically use classification algorythms if we are regressing a factor variable in the formula. It is still a good idea to specify the method outright.  

Use the following main types:  
- type = C-classification for classification  
- type = eps-regression for regression of numeric variables. 

Remember to do the usual data preparation; tokenize factors, scale variables. This will give you the best predictions.  

### Load the data

```{r}
data <- read.csv(file = "Part2_Regression/SVR/SVR/Position_Salaries.csv")
head(data)
```

### Create SVR regressor and prediction

```{r}
regressor <- svm(formula = Salary ~ .,data = data, type = "eps-regression")
y_pred <- predict(regressor, data) %>% data.frame()
```

### Plot the regression

```{r}
# Value_Prediction <-
#   predict(regressor, data.frame(Level = 6.5, Position = modelr::typical(data$Position)))

ggplot() +
  geom_point(data = data, aes(x = data$Level,
                                  y = data$Salary)) +
  geom_line(data = y_pred, aes(x = data$Level,
                               y = y_pred))
  # geom_point(data = y_pred, aes(x = 6.5,
  #                              y = Value_Prediction))

```

## Regression Trees

### Intuition

CART stands for classification and regression trees.  
There are, thereforem, 2 types of  trees (as usual).  

Watch the intuition video here: https://www.udemy.com/machinelearning/learn/v4/t/lecture/5732730?start=0  

Regression trees are based on information entropy. Entropy is a measure of uncertainty or the value of an additional data point in terms of information. 

Entropy is defined as $$E(-ln(P(X)))$$. It is often taken with base 2 in which case it is defined as bit entropy. Base 2 is computationally more efficient and often better leverages in binary outcomes. In the case of a bernouli process the entropy would be: $$\sum -P(x_i)ln(P(x_i)) = \sum -1/2ln(1/2)$$. The entropy is maximised when the odds are 1/2 because then we are the least certain  

The algorythm will devide the data into branches where the entropy is above a certain threshold. Once it has created enough branches it will make the regression prediction based on the average of the predictions of each branch depending on which branch the values lie for which you are predicting.  

### Load the data

```{r}
dataset <- 
read.csv(file = "Part2_Regression/Decision_Tree_Regression/Decision_Tree_Regression/Position_Salaries.csv")
```

### Run the regression

```{r}
regressor <- rpart(formula = Salary ~ ., data = dataset)

y_predict <- predict(regressor, newdata = dataset) %>% data.frame
```

### Plot

```{r}
ggplot() +
  geom_point(data = dataset, aes(x = dataset$Level,
                                  y = dataset$Salary)) +
  geom_line(data = y_predict, aes(x = dataset$Level,
                               y = y_predict))
```

There are no clear decision trees formed so the model  just took an average in 1 branch.  

Let's create more branches:

```{r}
regressor <- rpart(formula = Salary ~ ., data = dataset, control = rpart.control(minsplit = 1))

y_predict <- predict(regressor, newdata = dataset) %>% data.frame

ggplot() +
  geom_point(data = dataset, aes(x = dataset$Level,
                                  y = dataset$Salary)) +
  geom_line(data = y_predict, aes(x = dataset$Level,
                               y = y_predict))
```

In reality the decision tree is actually just giving an average for each branch. So it doesn't produce a smooth graph. It will return an average based on which branch/decision segment your independant predictor variables lie. To show this we predict not 1 point for each level but all point in an almost continuous sequence between the min and max value of level:

```{r}
x_grid <- seq(from = min(dataset$Level), to = max(dataset$Level), 0.01)

y_predict <- predict(regressor, newdata = data.frame(Level = x_grid)) %>% data.frame

ggplot() +
  geom_point(data = dataset, aes(x = dataset$Level,
                                  y = dataset$Salary)) +
  geom_line(data = y_predict, aes(x = x_grid,
                               y = y_predict))

```

## Random forest regression

Random forest is one method of ensemble learning. Generally speaking ensemble learning is fitting a model repeatedly and colating the results to create a more powerful and robust model.  

Random forest intuition:  
Pick n random points from the training set.  
Build a decision tree on these n points.  
Combine the trees to build a forest by predicting Y in each terminal mode and taking the average (either by counting out of total or predicting a probability in each tree and taking the mean).  

### Load data

```{r}
salary_data <- data.table::fread("Part2_Regression/Random_Forest_Regression/Position_Salaries.csv")
```

### Fit model

```{r}
library(randomForest)
predictor <- 
  randomForest::randomForest(data = salary_data,Salary~Level,ntree=10)
                # randomForest(
                             # data=salary_data,
                             # y = salary_data$Salary,
                             # x = salary_data$Level,
                             # ntree=10)
```

and now we predict an arbitrary value:

```{r}
y_predict <- predict(predictor, data.frame(Level = 6.5))
```

Let's plot this 2d relationship to understand the naive model:  

```{r}
x_range <- seq(min(salary_data$Level),max(salary_data$Level),0.01)

ggplot(data = salary_data, aes(x = Level,y = Salary), color = "red")+
  geom_point()+
  geom_smooth()+
  geom_line(data = data.frame(Level = x_range),aes(x=x_range,y=predict(predictor,newdata=data.frame(Level = Level))), color = "blue")

```

So in this simplified example we can nicely show how the random forest will attempt to predict the outcome variable in 2 dimensions. Obviously the model is much more powerful when working with larger and higher dimensional data. The randomForest package is particularly useful when explanetory variables are binary classes or multiple level factor variables since it has internal ways to dummify these cases and deal with them.

Pay particular attention to over fitting your data (test out of sample accuracy) and also to class imbalances when your variables are intricate.  

### A more robust application of machine learning regressions (random forest)

With real data the machine learning model will have to be trained and tested using a bit more rigor. In that case I suggest that the practitioner use the caret workflow.

Using the Iris dataset we illustrate:

```{r}
library(caret)
library(e1071)
classification_data = iris

cvCtrl = caret::trainControl(method = "LGOCV",
                             p = 0.8,
                             number = 10,
                             savePredictions = T)

caret_forest_model <-
      caret::train(data = classification_data, Species~., method = "rf", keep.forest=TRUE, trControl = cvCtrl
                   # , sampsize=10000,ntree=40
                   )

caret_forest_model %>% plot
varImp(caret_forest_model) %>% plot
caret_forest_model$results
```

So we got a 93% accuracy when classifying the species using 10 fold leave one out cross validation using a 80% traning split  

In this short pipeline you can split your data into training and testing , k-fold cross validate, using up or down sampling methods to deal with class imbalances, run on multiple cores and tune your parameters automatically or manually for a very large number of models
git 
The tuning of parameters can be done manually by setting:

```{r, eval=FALSE}
grid <- expand.grid(size=c(5,10,20,50), k=c(1,2,3,4,5))
model <- train(Species~., data=iris, method="lvq", trControl=control, tuneGrid=grid)
```

To run on multiple cores use the following workflow before training the model:

```{r}
  cores <- cores
  cl <- makeCluster(cores)
  registerDoParallel(cl)
  
  # ---> train
  
  stopCluster(cl)
```


# Part 4 - Clustering

# K-means

## Load the data

```{r}
dataset <-
  data.table::fread("Part_4_Clustering/K_Means/K_Means/Mall_Customers.csv")
  # read.csv(file = "Part_4_Clustering/K_Means/K_Means/Mall_Customers.csv")

```

## Find best number of clusters using elbow method

```{r}
subset <- 
  dataset %>% select(`Annual Income (k$)`, `Spending Score (1-100)`)

wcss <- vector()

for(i in 1:50){
  wcss[i] <- sum(kmeans(subset,i)$withinss)
}

wcss %>% plot

```

looks like ~5 clusters will give us a meaningfull number of clusters

## Create the clusters

On variables income, spend score:  

```{r}

cluster_1 <- kmeans(dataset %>% select(4:5), 5, iter.max = 1000, nstart = 10)

```

## Plot the clusters

```{r}
clusplot(
  dataset %>% select(4:5),
  cluster_1$cluster,
  lines = 0,
  shade = FALSE,
  color = TRUE,
  labels = 1,
  plotchar = FALSE,
  span = TRUE
  # main = "main title",
  # xlab = "x title",
  # ylab = "y title"
)
```

Apparently the k-means algorythm thinks these 2 columns describe all the variability. So a regression with only these factors should produce a good R^2   

Let's try adding some more columns and see how the plot and clustering algorythm behaves:  

_To include all the other variables we will have to tokenize the Genre column into a true/false 1/0 so that the algorythm can calculate euclidean distance, one might also want to normalize the variables_

```{r}

subset_2 <- 
  dataset %>% 
  select(Genre:`Spending Score (1-100)`) %>% 
  mutate(Genre = factor(Genre, labels = c(0,1),levels = c("Female","Male")))
  # mutate(Genre = factor(Genre, levels = c(0,1),labels = c("Female","Male")))


wcss_2 <- vector()

for(i in 1:50){
  wcss_2[i] <- sum(kmeans(subset_2,i)$withinss)
}

wcss_2 %>% plot

# Quicly look at points and gradients/skewness
wcss_2 %>% 
  tbl_df() %>% 
  arrange(-value) %>% 
  mutate(clusters = seq_along(value),
         gradient = lag(value)-value,
         gradient_rate_change = lag(gradient)-gradient) %>% 
  mutate_all(round)

# Using the 2nd derivitive as an aproximation we use 7 clusters as the optimal number

cluster_2 <- kmeans(subset_2, 7, iter.max = 1000, nstart = 10)

# clusplot(
#   subset_2,
#   cluster_2$cluster,
#   lines = 0,
#   shade = FALSE,
#   color = TRUE,
#   labels = 1,
#   plotchar = FALSE,
#   span = TRUE,
#   main = "main title",
#   xlab = "x title",
#   ylab = "y title"
# )
```

When we look at the kmeans output it has clearly used all the new columns. But the clusplot function can only plot 2 variables  

# Hierarchical clustering

There are broad approaches to H-clustering:  
  - Agglomerative  
  - Divisive  

_Agglomerative_ starts from the bottom and builds everything up (using a dendogram), and _Divisive_ is the opposite.    
  
_HC uses distance between clusters_.  
This can be defined in different ways since a cluster is a group of points!  

Steps:  
  1. Make a cluster for each point  
  2. Make n-1 clusters by grouping together the closest clusters  
  3. repeat step 2 untill you have 1 cluster left  
  
## Dendograms Agglomerative

A dendogram is a plot of distances for each cluster from another showing how clusters where connected given their distances  

For _Agglomerative_ the dendogram will build larger clusters from smaller clusters like building a tower. 

## Choosing the number of clusters using the dendogram

By looking at the dendogram we can determine the number of clusters we want to use by identifying some threshold on vertical distance that we do not want to cross.  

Here vertical distance refers to the distance between the clusters being joined on the current iteration 

_Rule of thumb:_ make the threshold somewhere in the height of the cluster with the largest vertical distance. Repeat this process untill the next vertical distance is not significantly larger than the previous

## Read in the data

```{r}
dataset <- 
  fread("Part_4_Clustering/Hierarchical-Clustering/Hierarchical_Clustering/Mall_Customers.csv")

subset <- 
  dataset %>% select(`Annual Income (k$)`,`Spending Score (1-100)`)
```

## Plot dendogram

method = "ward.D" will minimise each clusters within_variance

```{r}
HC <- hclust(d = dist(subset, method = "euclidean") ,
                    method = "ward.D")

HC %>% plot
# HC %>% plot(ylim=c(0, 200) )
```

We can clearly see from this dendogram that the threshold will be pretty low  

## Choose number of clusters

We now cut the dendogram tree and choos our number of clusters

```{r}
HC_clusters <- 
HC %>% cutree(k = 5) # 5 clusters
```

## Plot the HC results

```{r}
clusplot(
  subset,
  HC_clusters,
  lines = 0,
  shade = FALSE,
  color = TRUE,
  labels = 1,
  plotchar = FALSE,
  span = TRUE,
  main = "main title",
  xlab = "x title",
  ylab = "y title"
)
```

# Part 9 - Dimensionality Reduction

## Load data

```{r}
dataset <- 
  read.csv(file = "Part_9_Dimensionality_Reduction/PCA/Wine.csv")
```

## Overview

There are 2 ways of reducing the dimensions of your dataset before fitting a model:  
1. Feature selection. Like backward or forward elimination we did during Part 2 regression or using scores. This aims at removing features that are not significant in improving prediction in our dependant variable.  
2. Feature extraction. This aims at combining features into fewer variables (dimensions). This is the focus of this chapter.  

## PCA

Assume customer segment is the result of clustering customers on the independant variables... Using this data we could fit a classification model to better predict which customer segment will like a new wine that has certain values for these variables.

But in order to plot the link between the independant variables and the segment classification we would have to visualize too many variables.

Also, to inspect this using a table is very difficult when you have many variables.
Instead we use PCA to reduce the number of dimensions to 2 independant variables so that we can more easily tell a story of how customers are segmented (describe relationships that add customer insight).

### Split into test and train sets

```{r}
set.seed(123)
split <-  sample.split(dataset$Customer_Segment, SplitRatio = 0.8)
training_set <- dataset %>% subset(split)
test_set <- dataset %>% subset(!split)

```

### Feature scaling

```{r}
training_set[,1:13] <- scale(training_set[,1:13])
test_set[,1:13] <- scale(test_set[,1:13])
```

### Create PCA

#### Using Caret

If you want to reduce the dimensionality while explaining at least a certain threshold of variation in y you can set this threshold as:  
- thresh = 0.7  

If however you know how many features you want to extract use:  
- pcaComp = n  (this will overwrite thresh)  


```{r}
pca_caret <-
  preProcess(x = training_set[, -14],
  method = "pca",
  pcaComp = 2)
  
  prediction_caret <-
  pca_caret %>% predict(newdata = test_set[, -14])
  
  prediction_caret %>% plot()
```

#### Using FactoMineR

To set number of features here you use  
- ncp = n  

```{r}
pca_Facto <- 
  FactoMineR::PCA(X = training_set[,-14], 
                  ncp = 2)

predict_Facto <-
pca_Facto %>% predict(newdata = training_set[,-14])
 
pca_Facto %>% plot 
```


### Fit classification model

We can quicly fit a classification model to the feature scaled data. For simplicity we use simple models

```{r}
Classifier_glm <-
  glm(data = training_set,
  formula = Customer_Segment ~ .,
  family = "binomial")

Classifier_svm <-
  svm(x = training_set,
  type = "C-classification",
  y = dataset$Customer_Segment)

```

# Part 10 - Parameter Grid Search, Cross-validation and Boosting 

Load data:

```{r}
social_network_ads_data <- 
  data.table::fread("Part_10_Boosting/Model_Selection/Social_Network_Ads.csv")
```

## Cross validation

The purpose of crossvalidation is to see the average performance of the model accross different folds of the data. 

### Intuition

Cross-validation is essential when the practitioner believes that the modelling algorithms being employed is at risk of overfitting the data. In these cases cross-validation will split the data into different folds. By holding each fold out as the testing set the model can now test the out-of-sample accuracy looping through each fold while still leveraging all the rest of the data and then averaging out the model performance for these non-independent folds.

In practise it would be better to run multiple independant experiments, but this would be time consuming and we dont have time or money to run multiple experiments.

### How to

This can be interpreted via the confusion matrix. A perfect accuracy score would yield a confusion matrix where all entries are on the diagonal (i.e. if it was predicted a x_hat it was actualy x).  

The confusion matrix can be derived manually by using the table function. When used on a single vector the table function behaves like a group by count(*). When used on two vectors it will count the overlaps bewteen unique items in both sets.  

```{r}
library(caret)
library(e1071)

# a bottom up approach
folds = createFolds(iris, k = 10)
cv = lapply(folds, function(x){
  training_fold = iris[-x,]
  test_fold = iris[x,]
  classifier = svm(formula = Species ~ .,
                   data = training_fold,
                   type = 'C-classification',
                   kernel = 'radial'
                   )
  y_pred = predict(classifier,newdata = test_fold[,-5])
  outp = table(y_pred,test_fold$Species)
  # confusionMatrix(classifier)
})

# a more correct approach using built methods
  classifier = svm(formula = Species ~ .,
                   data = iris,
                   type = 'C-classification',
                   kernel = 'radial',
                   cross=10
                   )
  y_pred = predict(classifier,newdata = iris[,-5])
  outp = table(y_pred,iris$Species)
  outp
  
# A much better approach yet using the caret workbench
cvCtrl = caret::trainControl(method = "cv",
                             number = 10,
                             savePredictions = T)

caret_svm_model <-
      caret::train(data = iris, Species~.,
                   method = "svmRadial",
                   trControl = cvCtrl
                   )

caret_svm_model %>% plot
varImp(caret_svm_model) %>% plot
caret_svm_model$results

confusionMatrix(caret_svm_model)
```

Using the caret work bench we can leverage many powerfull functions and a more robust backend. Thus we get variable importance, properly executed cross-validation in the presence of other pre-processing operations and performance options such as parrallel computing.  

## Grid Search and Parameter Tuning

In machine learning applications there are 2 types of parameters:  
- Learned parameters that the machine learning algorithm will solve intuitively (e.g. coefficients in a regression model)  
- Hyper parameters that are chosen by the ML practitioner such as learning rate, tree size, test-train split etc.

One can imporve the performance of a model by finding the optimal combination of hyper-parameters by scanning through the parameter space. Generally speaking there are methods designed to improve the efficiency of finding these optima such as stochastic gradient decent. These and more advanced methods are especially employed in deep-learning models.  

As a basic starting point we can do a simple grid search to scan the hyper-parameter space. The idea here is to take every possible permutation of the parameters that can be used and running the model for each parameter in question until the optimal parameters are found for a scoring metric such as accuracy. 

Let's repeat the classification using svm but now we try to find the optimal hyper-parameters

```{r}
library(caret)

grid <- expand.grid(sigma=seq(0.1,0.9,length.out = 10), C=seq(0.1,1,length.out = 10))

cvCtrl = caret::trainControl(method = "cv",
                             number = 10,
                             savePredictions = T)

caret_svm_model <-
      caret::train(data = iris, Species~.,
                   method = "svmRadial",
                   trControl = cvCtrl,
                   tuneGrid=grid
                   )

caret_svm_model %>% plot()
varImp(caret_svm_model) %>% plot
caret_svm_model$results

confusionMatrix(caret_svm_model)
```

## XGBoost

XGBoost is an algorithm that has recently been dominating the machine learning literature where tabular or structured data is involved.  

It uses an implementation of decision trees using gradien boosting designed for speed and performance.  

In general the XGBoost algorithm is a more strict and optimized version of the GBM model in R.

```{r}
data_XGB = 
  # data.table::fread("Part_10_Boosting/XGBoost/Churn_Modelling.csv")
  readr::read_csv("Part_10_Boosting/XGBoost/Churn_Modelling.csv")
```

Pre-process the data

```{r}
data_XGB = data_XGB[,4:14]

#encode categorical variables as factors
data_XGB <- 
  data_XGB %>% 
  mutate(Geography = factor(Geography,
                            levels = c('France','Spain','Germany'),
                            labels = c(1,2,3)) %>% as.numeric(),
         Gender = factor(Gender,
                         levels = c('Female','Male'),
                         labels = c(1,2)) %>% as.numeric()
         )

library(caTools)
set.seed(123)

split = sample.split(data_XGB$Exited, SplitRatio = 0.8)
train = data_XGB[split,]
test = data_XGB[!split,]

```

no feature scaling required  

Train model:  

```{r}
library(xgboost)

classifier_XGB <- xgboost::xgboost(data = base::as.matrix(train[,-11] %>% tbl_df()),
                                   label = train$Exited,
                                   nrounds = 100) 

```

Manually calculate some accuracy statistics since this library does not seem to have relevant summary functions  

```{r}
y_pred = predict(classifier_XGB, newdata = as.matrix(test[,-11])) >=0.5
y_actual = test[,11] %>% flatten_dbl()

cm = table(y_pred,y_actual)

accuracy =  (cm[1,1] + cm[2,2]) / (cm[1,1] + cm[2,2] + cm[1,2] + cm[2,1]) #sum of diagonal over sum of all hits

# in other words the instructor should've used:
accuracy_ = sum(diag(cm)) / (test %>% nrow)
#since this can be used for any arbitrary number of categories
```

So interestingly; if I increase the parameter nrounds I can get the R-squared estimate down by a lot, especially since the algorithm is so fast. However, this actually decreases the percieved accuracy of the model. That's interesting, also why does the model report R-squared anyway, if it is not a relevant measure of accuracy?

Let's apply our grid_search approach to find a better parameter value:  

### Manual grid-search approach (1 parameter)

For multiple parameters use expand.grid together with pmap (exponentially taxing)  

```{r}
# grid = expand.grid()
grid = seq(1,5000,length.out = 19) # find the best parameter based on n runs

grid_accuracy = grid %>% map(~xgboost::xgboost(data = base::as.matrix(train[,-11] %>% tbl_df()),
                               label = train$Exited,
                               nrounds = .x)) %>%
  map(~predict(.x, newdata = as.matrix(test[,-11])) >=0.5) %>% 
  map(~table(test[,11] %>% flatten_dbl(),.x)) %>% 
  map(~sum(diag(.x)) / (test %>% nrow))

grid_accuracy %>% flatten_dbl() %>% plot
```

Fascinating, the actual accuracy goes down the more iterations you use...
I expected some sort of cut-off where overfitting happens but its entirely a downward trend. This casts a lot of doubt on the actual validity of the model being used. Not because it's bad but because it may be being used incorrectly by the people in this course. Alas I don't know enough about this model to really say whats wrong here, but its suspicious.

The model does however converge on about 1300 iterations with accuracy of roughly ~0.85 and RMSE of 0.004094. Interestingly the max accuracy ~0.87 has model specified RMSE of ~0.5 which could indicate a few outliers skewing the model accuracy when optimized using RMSE.

If this is true the model using gradient descent performs extremely well after only a single iteration. That implies that the default starting parameters for the tree based search happens to be pretty good or the gradient descent arrived at a good solution of hyper-parameters after only a single iteration.

# Playground

```{r}


```

